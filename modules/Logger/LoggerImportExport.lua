local _, BPGP = ...

local Logger = BPGP.GetModule("Logger")

local L = LibStub:GetLibrary("AceLocale-3.0"):GetLocale("BPGP")
local LibJSON = LibStub("LibJSON-1.0")
local LibDeflate = LibStub("LibDeflate")

local Common = BPGP.GetLibrary("Common")

local DataManager = BPGP.GetModule("DataManager")

function Logger.Export()
  local data = {}
  data.region = GetCVar("portal")
  data.realm = GetRealmName()
  data.guild = DataManager.GetGuildName()
  data.origin = DataManager.GetSelfName()
  data.timestamp = Logger.GetTimestamp()
  data.log = {}
  
  local targetTable = Logger.GetFilterTable()
  
  for tableName, dbRecords in pairs(Logger.db.profile.cache.logs) do
    if tableName == targetTable or targetTable == L["All"] then
      if not data.log[tableName] then data.log[tableName] = {} end
      for i, dbRecord in ipairs(dbRecords) do
        table.insert(data.log[tableName], Logger.SerializeDBRecord(dbRecord))
      end
    end
  end

  local jsonData = LibJSON.Serialize(data)
  local compressedData = LibDeflate:CompressDeflate(jsonData, {["level"] = 5})
  local encodedData = LibDeflate:EncodeForPrint(compressedData)
  
  BPGP.Print(L["Done log export from table(s) '%s' (raw: %d bytes, compressed: %d bytes)."]:format(targetTable, #jsonData, #encodedData))

  return encodedData
end

function Logger.ParseData(compressedJson)
  local err = nil
  local decodedData = LibDeflate:DecodeForPrint(compressedJson)
  if not decodedData then
    err = L["Error: failed to decode compressed data!"].."\n"..L["Please make sure you have copied the paste properly."]
    return nil, err
  end
  local decompressedData = LibDeflate:DecompressDeflate(decodedData, {["level"] = 5})
  if not decompressedData then
    err = L["Error: failed to decompress data!"].."\n"..L["Please make sure you have copied the paste properly."]
    return nil, err
  end
  local success, data = pcall(LibJSON.Deserialize, decompressedData)
  if not success then
    err = L["Error: failed to parse decompressed data!"].."\n"..L["It shouldn't normally happen. You need a new paste."]
    return nil, err
  end
  if data.region ~= GetCVar("portal") then
    err = L["Error: got region %s, expected %s!"]:format(data.region, GetCVar("portal"))
  elseif data.realm ~= GetRealmName() then
    err = L["Error: got realm %s, expected %s!"]:format(data.realm, GetRealmName())
  elseif data.guild ~= DataManager.GetGuildName() then
    err = L["Error: got guild %s, expected %s!"]:format(data.guild, DataManager.GetGuildName())
  elseif data.timestamp > Logger.GetTimestamp() then
    err = L["Error: data is from the future (%s)!"]:format(date("%Y-%m-%d %H:%M:%S", data.timestamp))
  elseif type(data.log) ~= "table" then
    err = L["Error: invalid log data type (%s)!"]:format(type(data.log))
  end
  return data, err
end

-- Imports paste generated by Export function
-- Tries to import as many valid log records as possible while ignoring ones with corrupted data and reporting weird ones
-- Every ML actions log is literally blockchain, where hash of every next record is built from its values as well as the hash of the previous record, which allows to easly detect any missing log fragments or data manipulations. Good ML's log is uniterrupted log from the start up to the end.
function Logger.Import(data)
  BPGP.Print(L["Starting log import..."])
  
  -- Log chains merging and pre-processing
  local mergedLogChains, numNewRecords = {}, 0
  for tableName, serializedRecords in pairs(data.log) do
    if not mergedLogChains[tableName] then mergedLogChains[tableName] = {} end
    -- Caching records from already known log chains
    local existingRecords = {}
    for i = 1, #Logger.db.profile.cache.logs[tableName] do
      local dbRecord = Logger.db.profile.cache.logs[tableName][i]
      existingRecords[dbRecord[1]] = true
      table.insert(mergedLogChains[tableName], dbRecord)
    end
    -- Caching new records from imported log chains
    local numNewTableRecords = 0
    for i = 1, #serializedRecords do
      local dbRecord = Logger.DeserializeDBRecord(serializedRecords[i])
      if #dbRecord == 4 then
        if not existingRecords[dbRecord[1]] then
          table.insert(mergedLogChains[tableName], dbRecord)
          numNewTableRecords = numNewTableRecords + 1
        end
      else
        BPGP.Print(L["Ignored invalid log record #%d: %s"]:format(i, serializedRecords[i]))
      end
    end
    if numNewTableRecords > 0 then
      table.sort(mergedLogChains[tableName], function(a, b) return a[1] < b[1] end)
      BPGP.Print(L["Found %d new records for %s table."]:format(numNewTableRecords, tableName))
    else
      table.wipe(mergedLogChains[tableName])
      BPGP.Print(L["No new records found for %s table."]:format(tableName))
    end
    numNewRecords = numNewRecords + numNewTableRecords
  end
  if numNewRecords == 0 then
    BPGP.Print(L["Log import aborted: no new records found!"])
    return
  end
  
  -- Merged chains analyzis and error reporting
  local logChainsHashes, headHash = {}, Logger.GetHeadHash()
  for tableName, mergedLogChain in pairs(mergedLogChains) do
    if not logChainsHashes[tableName] then logChainsHashes[tableName] = {} end
    local logChainsTimestamps = {}
    BPGP.Print(L["Analyzing log records for %s table..."]:format(tableName))
    for i = 1, #mergedLogChain do
      -- Imported record data unpacking
      local guid, value, target, reason = unpack(mergedLogChain[i])
      local _, timestamp, kind, mode, origin, hash = Logger.DecodeDBRecordGUID(guid)
      -- Write initial hash if new ML log chain found
      if not logChainsHashes[tableName][origin] then
        logChainsHashes[tableName][origin] = headHash
      end
      -- Every record hash for ML's log is calculated based on the previous one, we should always fetch it
      local chainHash = logChainsHashes[tableName][origin]
      -- We want to reconstruct the record to make sure that every value at least passes basic checks
      local success, dbRecord = pcall(Logger.NewDBRecord, nil, tableName, kind, mode, origin, value, target, reason, timestamp, chainHash)
      if success then
        local dbRecordDescriptor = {Logger.DecodeDBRecordGUID(dbRecord[1])}
        
        local datetime = date("%Y-%m-%d %H:%M:%S", dbRecordDescriptor[2]:sub(1, -4))
        
        if hash == dbRecordDescriptor[6] then
          -- Record hash is valid. Here we notifying the user and updating latest chain hash state
          if chainHash == headHash then
            -- This is valid head record, under normal circumstances it means this is first log record from this ML
            -- Less likely case it's first log record after ML's log wipe imported by user without deep enough log history
            BPGP.Print(L["Found %s's log start at %s."]:format(origin, datetime))
  --        else
            -- It's just a regular valid log record. There's no need to spam the user with it
          end
          -- Record is valid and is on top of the processed chain, next record should inherit from this one
          logChainsHashes[tableName][origin] = dbRecordDescriptor[6]
        else
          -- Record hash is not valid. Something is wrong, we need to warn the user and try to handle this case somehow
          if chainHash == headHash then
            -- This is head record with invalid hash. Most likely it's a head of exported chain fragment
            BPGP.Print(L["Missing %s's log records before %s."]:format(origin, datetime))
            -- We will try to build the chain inheriting from this record's hash, not from calculated one
            logChainsHashes[tableName][origin] = hash
          else
            local newHash = Logger.GetRecordHash(tableName, timestamp, kind, mode, origin, value, target, reason, headHash)
            if newHash == dbRecordDescriptor[6] then
              -- We're somewhere deep inside log chain and faced new valid head record, ML most likely had log data loss
              BPGP.Print(L["Found %s's new log start at %s. Why did he wipe his log?"]:format(tableName, datetime))
              logChainsHashes[tableName][origin] = dbRecordDescriptor[6]
            else
              -- Probably it's a head of exported chain fragment.
              BPGP.Print(L["Missing %s's log records between %s and %s."]:format(origin, logChainsTimestamps[origin], datetime))
              -- We will try to build the chain inheriting from this record's hash, not from calculated one
              logChainsHashes[tableName][origin] = hash
            end
          end
        end
        logChainsTimestamps[origin] = datetime
      else
        BPGP.Print(L["Ignored invalid log record #%d: %s"]:format(i, dbRecord:match(".-:%d+: (.+)")))
      end
    end
    -- Here we just let the user know about the last records timestamps
    for origin, timestamp in pairs(logChainsTimestamps) do
      BPGP.Print(L["Found %s's log end at %s."]:format(origin, timestamp))
    end
  end
  
  -- We're almost done with import, the only thing left is to dump merged chains to the storage
  BPGP.Print(L["Saving imported data..."])
  for tableName, mergedLogChain in pairs(mergedLogChains) do
    if #mergedLogChain > 0 then
      Logger.WipeTableLog(tableName)
      Logger.WipeBufferedLog(tableName)
      for i = 1, #mergedLogChain do
        Logger.WriteDBRecord(tableName, mergedLogChain[i])
      end
    end
  end
  
  -- Finally!
  BPGP.Print(L["Log import finished!"])
end
